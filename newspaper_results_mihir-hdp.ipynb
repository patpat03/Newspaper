{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lovely-starter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6196692706606616\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#Set Seed\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(2020)\n",
    "print(random.random())\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#Detect number of cores on computer (needed for multicore processing to speed up code)\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "import psutil\n",
    "psutil.cpu_count()\n",
    "psutil.cpu_count(logical=False)  # Ignoring virtual cores\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#Set working directory\n",
    "#------------------------------------------------------------------------------\n",
    "import os\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#Import relevant packages\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from pprint import pprint\n",
    "import csv\n",
    "\n",
    "from dateutil.parser import parse\n",
    "from pandas import DataFrame\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "macro-ozone",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/usr/local/lib/python3.9/site-packages/docx/section.py:7: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#Prepare stopwords\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "#stop_words.extend(['using', 'publication', 'ltd', 'elsevier', 'reserved', 'rights'])\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#Import newspaper articles\n",
    "#------------------------------------------------------------------------------\n",
    "import glob\n",
    "import docx\n",
    "\n",
    "#define blank list\n",
    "words = []\n",
    "\n",
    "#define function to import USAT, NYT, WaPo\n",
    "def list_files(dir):                                                                                                  \n",
    "    r = []                                                                                                            \n",
    "    subdirs = [x[0] for x in os.walk(dir)]                                                                            \n",
    "    for subdir in subdirs:                                                                                            \n",
    "        files = os.walk(subdir).__next__()[2]                                                                             \n",
    "        if (len(files) > 0):                                                                                          \n",
    "            for file in files:\n",
    "                if \"Bibliography\" in file:\n",
    "                    continue\n",
    "                if(\"doclist\" in file):\n",
    "                    continue\n",
    "                if(\".docx\" in file):\n",
    "                    r.append(os.path.join(subdir, file))                                                                         \n",
    "    return r                                                                                                          \n",
    "\n",
    "\n",
    "file_list = list_files(\"/Users/Mihir/Downloads/Sample/\")\n",
    "#file_list = list_files(\"/Users/anavirshermon/Dropbox (Kenan-Flagler)/Newspaper Analysis/Sample/\")\n",
    "\n",
    "#import wsj articles, which are in an excel sheet\n",
    "wsj      = pd.read_excel(\"/Users/Mihir/Downloads/Sample/wsj.xlsx\", engine='openpyxl') \n",
    "wsj      = wsj[wsj['Content'].notna()]\n",
    "\n",
    "wsj['year'] = wsj['date'].dt.year #extract year\n",
    "\n",
    "wsj_1    = wsj[(wsj['year']<=2012)]\n",
    "wsj_2    = wsj[(wsj['year']>2012) & (wsj['year']<= 2015)]\n",
    "wsj_3    = wsj[(wsj['year']>=2016)]\n",
    "\n",
    "wsj_list   = wsj['Content'].tolist()\n",
    "wsj_1_list = wsj_1['Content'].tolist()\n",
    "wsj_2_list = wsj_2['Content'].tolist()\n",
    "wsj_3_list = wsj_3['Content'].tolist()\n",
    "\n",
    "    \n",
    "#add words to files list\n",
    "for i in file_list:\n",
    "    doc = docx.Document(i)\n",
    "    words.append([p.text for p in doc.paragraphs]) \n",
    "        \n",
    "#making each word in paragraph formation and removing all words that come before \"Body\", which tells us when the paragraph begins\n",
    "for i in range(len(words)):\n",
    "    words[i] = ' '.join(words[i])\n",
    "    #words[i] = words[i][words[0].index(\"Body \"):]\n",
    "    \n",
    "one = []\n",
    "two = [] \n",
    "three = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cloudy-vietnamese",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#split up files between dates\n",
    "for i in words:\n",
    "    date =parse(i[i.index(\"Load-Date:\") + 11:i.index(\"End of Document\")])\n",
    "    if(date.year<=2012):\n",
    "        one.append(i)\n",
    "    elif(date.year<=2015):\n",
    "        two.append(i)\n",
    "    else:\n",
    "        three.append(i)\n",
    "\n",
    "words_cleaned = [x.rsplit(\"Body \")[1] for x in words]\n",
    "words_cleaned = [x.split(\"Load-Date\")[0] for x in words_cleaned]\n",
    "words_cleaned = [x for x in words_cleaned if str(x) != 'nan']\n",
    "\n",
    "one_cleaned = [x.rsplit(\"Body \")[1] for x in one]\n",
    "one_cleaned = [x.split(\"Load-Date\")[0] for x in one_cleaned]\n",
    "one_cleaned = [x for x in one_cleaned if str(x) != 'nan']\n",
    "\n",
    "two_cleaned = [x.rsplit(\"Body \")[1] for x in two]\n",
    "two_cleaned = [x.split(\"Load-Date\")[0] for x in two_cleaned]\n",
    "two_cleaned = [x for x in two_cleaned if str(x) != 'nan']\n",
    "\n",
    "three_cleaned = [x.rsplit(\"Body \")[1] for x in three]\n",
    "three_cleaned = [x.split(\"Load-Date\")[0] for x in three_cleaned]\n",
    "three_cleaned = [x for x in three_cleaned if str(x) != 'nan']\n",
    "\n",
    "for i in wsj_list:\n",
    "    words_cleaned.append(i)  \n",
    "\n",
    "for i in wsj_1_list:\n",
    "    one_cleaned.append(i)\n",
    "    \n",
    "for i in wsj_2_list:\n",
    "    two_cleaned.append(i)\n",
    "    \n",
    "for i in wsj_3_list:\n",
    "    three_cleaned.append(i)\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#Drop military related newspaper articles\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "#Code to eventually use tf-idf or some systematic method to drop military articles\n",
    "from pandas import DataFrame\n",
    "import re\n",
    "\n",
    "#drop articles if they contain these words\n",
    "# =============================================================================\n",
    "# drop_words    = {'military', 'strike', 'attack', 'syria', 'iran', 'pakistan'}\n",
    "# words_cleaned = [i for i in words_cleaned if not any(x in i for x in drop_words)]\n",
    "# one_cleaned   = [i for i in one_cleaned if not any(x in i for x in drop_words)]\n",
    "# two_cleaned   = [i for i in two_cleaned if not any(x in i for x in drop_words)]\n",
    "# two_cleaned = [i for i in three_cleaned if not any(x in i for x in drop_words)]\n",
    "# \n",
    "# =============================================================================\n",
    "\n",
    "drop_words= {'military', 'strike', 'attack', 'syria', 'yemen', 'pakistan', 'afghanistan', 'taliban', 'pentagon', 'islamic'}\n",
    "\n",
    "#convert to lower case\n",
    "data        = [item.lower() for item in words_cleaned]\n",
    "data_one    = [item.lower() for item in one_cleaned]\n",
    "data_two    = [item.lower() for item in two_cleaned]\n",
    "data_three  = [item.lower() for item in three_cleaned]\n",
    "\n",
    "#remove new line characters\n",
    "#data = [re.sub('\\s+', ' ', sent) for sent in data]   # Remove new line characters\n",
    "\n",
    "#tokenize and pre-process the text\n",
    "def split(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "data        = list(split(data))\n",
    "data_one    = list(split(data_one))\n",
    "data_two    = list(split(data_two))\n",
    "data_three  = list(split(data_three))\n",
    "\n",
    "\n",
    "#remove numbers, but not words that contain numbers\n",
    "data       = [[token for token in doc if not token.isnumeric()] for doc in data]\n",
    "data_one   = [[token for token in doc if not token.isnumeric()] for doc in data_one]\n",
    "data_two   = [[token for token in doc if not token.isnumeric()] for doc in data_two]\n",
    "data_three = [[token for token in doc if not token.isnumeric()] for doc in data_three]\n",
    "\n",
    "#remove words that are only two characters\n",
    "data       = [[token for token in doc if len(token) > 2] for doc in data]\n",
    "data_one   = [[token for token in doc if len(token) > 2] for doc in data_one]\n",
    "data_two   = [[token for token in doc if len(token) > 2] for doc in data_two]\n",
    "data_three = [[token for token in doc if len(token) > 2] for doc in data_three]\n",
    "\n",
    "#remove stopwords\n",
    "def remove_stopwords(words):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in words]\n",
    "data       = remove_stopwords(data)\n",
    "data_one   = remove_stopwords(data_one)\n",
    "data_two   = remove_stopwords(data_two)\n",
    "data_three = remove_stopwords(data_three)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# from collections import Counter \n",
    "# test = Counter(c for clist in data for c in clist)\n",
    "# most_occur = test.most_common(150)\n",
    "# \n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "front-miniature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2498\n",
      "668\n",
      "1121\n",
      "709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(len(data_one))\n",
    "print(len(data_two))\n",
    "print(len(data_three))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "veterinary-cambridge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nouns = {x.name().split('.', 1)[0] for x in wn.all_synsets('n')}\n",
    "verbs = {x.name().split('.', 1)[0] for x in wn.all_synsets('v')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "related-eligibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i] = [w for w in data[i] if w in nouns]\n",
    "for i in range(len(data_one)):\n",
    "    data_one[i] = [w for w in data_one[i] if (w in nouns or w in verbs)]\n",
    "for i in range(len(data_two)):\n",
    "    data_two[i] = [w for w in data_two[i] if (w in nouns or w in verbs)]\n",
    "for i in range(len(data_three)):\n",
    "    data_three[i] = [w for w in data_three[i] if (w in nouns or w in verbs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "whole-mouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "'''\n",
    "These lines combine all of the data into paragraph form so that it can be read by the tfidf vector class. They are then split back afterwards.\n",
    "'''\n",
    "data = [' '.join(w) for w in data]\n",
    "data_one = [' '.join(w) for w in data_one]\n",
    "data_two = [' '.join(w) for w in data_two]\n",
    "data_three = [' '.join(w) for w in data_three]\n",
    "loop = [data, data_one, data_two, data_three]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "square-rough",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "names = [\"data\", \"data_one\", \"data_two\", \"data_three\"]\n",
    "group = [data, data_one, data_two, data_three]\n",
    "d = {}\n",
    "'''\n",
    "Creates the td-idf vectors for the total (data), 2012-2015 (data_one), 2015-2017 (data_two), 2017+(data_three)\n",
    "and stores in the a dictionary (d)\n",
    "'''\n",
    "for i, j in zip(names, group):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(j)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df = pd.DataFrame(denselist, columns=feature_names)\n",
    "    d[i] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pointed-multiple",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Splits data again \n",
    "'''\n",
    "data = [i.split() for i in data]\n",
    "data_one = [i.split() for i in data_one]\n",
    "data_two = [i.split() for i in data_two]\n",
    "data_three = [i.split() for i in data_three]\n",
    "res = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rotary-collaboration",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#Gets the top 40 words for each of the tf-idf vectors and stores it in the dictionary (res)\n",
    "for n, i in zip(names, d):    \n",
    "    array = []\n",
    "    for j in d[i].iterrows():\n",
    "         array.append((j[1].sort_values(ascending=False)[:40].keys().tolist()))\n",
    "    res[n] = array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "significant-first",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#Drops document if the top 40 words is in drop_words array\n",
    "data = [data[i] for i in range(len(data)) if not any (x in res['data_one'][i] for x in drop_words)]\n",
    "data_one = [data_one[i] for i in range(len(data_one)) if not any (x in res['data_one'][i] for x in drop_words)]\n",
    "data_two = [data_two[i] for i in range(len(data_two)) if not any (x in res['data_two'][i] for x in drop_words)]\n",
    "data_three = [data_three[i] for i in range(len(data_three)) if not any (x in res['data_three'][i] for x in drop_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "overall-basin",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#Saves data\n",
    "pd.DataFrame(res['data']).to_csv(\"total.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cathedral-chaos",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# coding: utf-8\n",
    "\n",
    "## Author: Eduardo Coronado (Duke University)\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import sys\n",
    "\n",
    "def train_HDPmodel(hdp, word_list, mcmc_iter, burn_in=100, quiet=False):\n",
    "    '''Wrapper function to train tomotopy HDP Model object\n",
    "    \n",
    "    *** Inputs**\n",
    "    hdp: obj -> initialized HDPModel model\n",
    "    word_list: list -> lemmatized word list of lists\n",
    "    mcmc_iter : int -> number of iterations to train the model\n",
    "    burn_in: int -> MC burn in iterations\n",
    "    quiet: bool -> flag whether to print iteration LL and Topics, if True nothing prints out\n",
    "    \n",
    "    ** Returns**\n",
    "    hdp: trained HDP Model \n",
    "    '''\n",
    "    \n",
    "    # Add docs to train\n",
    "    for vec in word_list:\n",
    "        hdp.add_doc(vec)\n",
    "\n",
    "    # Initiate MCMC burn-in \n",
    "    hdp.burn_in = 100\n",
    "    hdp.train(0)\n",
    "    print('Num docs:', len(hdp.docs), ', Vocab size:', hdp.num_vocabs, ', Num words:', hdp.num_words)\n",
    "    print('Removed top words:', hdp.removed_top_words)\n",
    "    print('Training...', file=sys.stderr, flush=True)\n",
    "\n",
    "    # Train model\n",
    "    step=round(mcmc_iter*0.10)\n",
    "    for i in range(0, mcmc_iter, step):\n",
    "        hdp.train(step, workers=3)\n",
    "        if not quiet:\n",
    "            print('Iteration: {}\\tLog-likelihood: {}\\tNum. of topics: {}'.format(i, hdp.ll_per_word, hdp.live_k))\n",
    "        \n",
    "    print(\"Done\\n\")  \n",
    "    \n",
    "    return hdp\n",
    "    \n",
    "        \n",
    "def get_hdp_topics(hdp, top_n=10):\n",
    "    '''Wrapper function to extract topics from trained tomotopy HDP model \n",
    "    \n",
    "    ** Inputs **\n",
    "    hdp:obj -> HDPModel trained model\n",
    "    top_n: int -> top n words in topic based on frequencies\n",
    "    \n",
    "    ** Returns **\n",
    "    topics: dict -> per topic, an arrays with top words and associated frequencies \n",
    "    '''\n",
    "    \n",
    "    # Get most important topics by # of times they were assigned (i.e. counts)\n",
    "    sorted_topics = [k for k, v in sorted(enumerate(hdp.get_count_by_topics()), key=lambda x:x[1], reverse=True)]\n",
    "\n",
    "    topics=dict()\n",
    "    \n",
    "    # For topics found, extract only those that are still assigned\n",
    "    for k in sorted_topics:\n",
    "        if not hdp.is_live_topic(k): continue # remove un-assigned topics at the end (i.e. not alive)\n",
    "        topic_wp =[]\n",
    "        for word, prob in hdp.get_topic_words(k, top_n=top_n):\n",
    "            topic_wp.append((word, prob))\n",
    "\n",
    "        topics[k] = topic_wp # store topic word/frequency array\n",
    "        \n",
    "    return topics\n",
    "\n",
    "\n",
    "\n",
    "def eval_coherence(topics_dict, word_list, coherence_type='c_v'):\n",
    "    '''Wrapper function that uses gensim Coherence Model to compute topic coherence scores\n",
    "    \n",
    "    ** Inputs **\n",
    "    topic_dict: dict -> topic dictionary from train_HDPmodel function\n",
    "    word_list: list -> lemmatized word list of lists\n",
    "    coherence_typ: str -> type of coherence value to comput (see gensim for opts)\n",
    "    \n",
    "    ** Returns **\n",
    "    score: float -> coherence value\n",
    "    '''\n",
    "    \n",
    "    # Build gensim objects\n",
    "    vocab = corpora.Dictionary(word_list)\n",
    "    corpus = [vocab.doc2bow(words) for words in word_list]\n",
    "    \n",
    "    # Build topic list from dictionary\n",
    "    topic_list=[]\n",
    "    for k, tups in topics_dict.items():\n",
    "        topic_tokens=[]\n",
    "        for w, p in tups:\n",
    "            topic_tokens.append(w)\n",
    "            \n",
    "        topic_list.append(topic_tokens)\n",
    "            \n",
    "\n",
    "    # Build Coherence model\n",
    "    print(\"Evaluating topic coherence...\")\n",
    "    cm = CoherenceModel(topics=topic_list, corpus=corpus, dictionary=vocab, texts=word_list, \n",
    "                    coherence=coherence_type)\n",
    "    \n",
    "    score = cm.get_coherence()\n",
    "    print (\"Done\\n\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "durable-monkey",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['administration', 'accounting', 'noncombatant', 'statistic', 'may', 'much', 'left', 'release_information', 'first', 'three', 'month', 'part', 'president', 'use_force', 'battle', 'terrorism', 'operation', 'speech_national_defense', 'university', 'subject', 'transparency_oversight', 'death', 'like', 'cover', 'war', 'yemen_somalia_libya', 'pakistan', 'drone', 'administration', 'part', 'afghanistan', 'war', 'theater', 'still', 'inside_pakistan', 'pentagon', 'saturday', 'taliban_leader', 'pakistan', 'must', 'level', 'government', 'special_operations', 'three', 'self_defense', 'defense_secretary', 'case', 'march', 'somalia', 'aircraft', 'training_camp', 'war', 'strike', 'may', 'african', 'peacekeeping', 'west', 'mogadishu', 'somali', 'capital', 'keep', 'administration', 'civilian', 'drone', 'statistic', 'threat', 'growth', 'state', 'standing', 'president', 'term', 'january', 'hope', 'time', 'leave', 'office', 'structure', 'place', 'set', 'process', 'people', 'look', 'government', 'basis', 'last_month', 'announcement', 'information', 'executive_order', 'senior_administration', 'spoke_condition_anonymity', 'information', 'public', 'set', 'future', 'system', 'making', 'bind', 'time', 'constitutional', 'change', 'robert', 'associate', 'dean', 'director_robert', 'strauss', 'center_international', 'security', 'law', 'university_texas', 'law_school', 'possible', 'matter', 'one', 'administration', 'binding', 'sense', 'predecessor', 'george_bush', 'time', 'drone', 'weapon_choice', 'pakistan', 'bush_administration', 'president', 'top', 'program', 'international_law', 'secrecy', 'pakistan', 'covert', 'way', 'civilian', 'government', 'dilemma', 'news_conference', 'last_month', 'saying', 'past', 'criticism', 'architecture', 'use', 'drone', 'doubt', 'beginning', 'right', 'time', 'end', 'iraq', 'war', 'withdrawal_afghanistan', 'senior_administration', 'need', 'drone', 'goal', 'term', 'framework', 'occasion', 'operation', 'pakistan', 'ending', 'business', 'sole', 'responsibility', 'military', 'war', 'outside_active_hostilities', 'policy', 'term', 'international', 'war', 'concept', 'world', 'subject', 'sharp', 'debate', 'international_law', 'variety', 'domestic_international', 'authorization_use_military', 'force', 'congress', 'force', 'year', 'government', 'decade', 'list', 'bush', 'yemen_arabian_peninsula', 'subject', 'administration', 'state', 'part', 'despite', 'bitter', 'congress', 'cover', 'state', 'article', 'constitution', 'commander', 'country', 'provision', 'bush', 'beginning', 'administration', 'international_law', 'ally', 'event', 'attack', 'government', 'preventive', 'defense', 'administration', 'law', 'threat', 'much', 'universe', 'former', 'president', 'given', 'certainty', 'civilian', 'summary', 'terrorism', 'landscape', 'administration', 'keep', 'two', 'iraq_syria', 'list', 'war', 'military', 'international', 'war', 'state', 'born', 'spread', 'middle_east', 'north_africa', 'refining', 'strengthening', 'action', 'terrorist', 'outside_active_hostilities', 'senior_administration', 'official', 'policy_guidance', 'number', 'three', 'administration', 'pas', 'reduction', 'pakistan', 'number', 'high', 'two', 'outside', 'year', 'saturday', 'military', 'strike', 'focus', 'outside_war', 'yemen', 'number', 'fell', 'two', 'rise', 'year', 'somalia', 'first', 'two', 'terrorist', 'year', 'libya', 'first', 'joint', 'air', 'european_allies', 'outside', 'government', 'number', 'separate', 'category', 'defense', 'special_operations', 'ground', 'fact', 'defense_department', 'central', 'list', 'outside_active_hostilities', 'pentagon', 'central_command', 'charge', 'yemen', 'africa_command', 'public', 'security', 'response', 'drone', 'secret', 'still', 'thought', 'yemen', 'beginning', 'november', 'official', 'half', 'half', 'yemen', 'two', 'libya', 'rest', 'somalia', 'journalism', 'one', 'count', 'local', 'civilian', 'individual', 'operation', 'zero', 'low', 'single', 'transparency', 'defense_department', 'real_time', 'information', 'outside_active_hostilities', 'release', 'future', 'annual', 'tate_report_classification_language', 'english_publication_type_newspaper', 'subject_war_conflict', 'islam_death', 'terrorism', 'taliban', 'defense', 'peacekeeping', 'executive', 'government', 'religion_organization', 'national_defense', 'university', 'industry', 'military', 'aircraft', 'defense', 'aircraft', 'person', 'somalia', 'somalia', 'pakistan', 'libya', 'afghanistan', 'africa']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#create bigrams and trigrams\n",
    "bigram         = gensim.models.Phrases(data, min_count=5, threshold=10)\n",
    "trigram        = gensim.models.Phrases(bigram[data], threshold=10)\n",
    "bigram_mod     = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod    = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "data_bigrams    = make_bigrams(data)\n",
    "data_trigrams   = make_trigrams(data)\n",
    "\n",
    "data_one_trigrams   = make_trigrams(data_one)\n",
    "data_two_trigrams   = make_trigrams(data_two)\n",
    "data_three_trigrams = make_trigrams(data_three)\n",
    "\n",
    "#lemmatize words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data_lemmatized = [[lemmatizer.lemmatize(token) for token in doc] for doc in data_trigrams]\n",
    "\n",
    "print(data_lemmatized[99:100])\n",
    "\n",
    "data_one_lemmatized   = [[lemmatizer.lemmatize(token) for token in doc] for doc in data_one_trigrams]\n",
    "data_two_lemmatized   = [[lemmatizer.lemmatize(token) for token in doc] for doc in data_two_trigrams]\n",
    "data_three_lemmatized = [[lemmatizer.lemmatize(token) for token in doc] for doc in data_three_trigrams]\n",
    "\n",
    "data_lemmatized = data_one_lemmatized\n",
    "\n",
    "group = \"_2013-2015\"\n",
    "\n",
    "id2word = gensim.corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "id2word.filter_extremes(no_below=20, no_above=0.75)\n",
    "\n",
    "texts = data_lemmatized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "third-tablet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "data_lemmatized = data_three_lemmatized\n",
    "texts = data_lemmatized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "reserved-inspection",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import tomotopy as tp\n",
    "term_weight = tp.TermWeight.ONE\n",
    "hdp = tp.HDPModel(tw=term_weight, min_cf=5, rm_top=7, gamma=1,\n",
    "                  alpha=0.1, initial_k=10, seed=99999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "coated-skill",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num docs: 446 , Vocab size: 2800 , Num words: 68011\n",
      "Removed top words: ['drone', 'one', 'company', 'technology', 'use', 'aircraft', 'people']\n",
      "Iteration: 0\tLog-likelihood: -7.630016002346397\tNum. of topics: 31\n",
      "Iteration: 100\tLog-likelihood: -7.573190233157086\tNum. of topics: 35\n",
      "Iteration: 200\tLog-likelihood: -7.568108922212357\tNum. of topics: 34\n",
      "Iteration: 300\tLog-likelihood: -7.557782638576195\tNum. of topics: 38\n",
      "Iteration: 400\tLog-likelihood: -7.558150741676118\tNum. of topics: 38\n",
      "Iteration: 500\tLog-likelihood: -7.550722510197929\tNum. of topics: 41\n",
      "Iteration: 600\tLog-likelihood: -7.548508014237391\tNum. of topics: 38\n",
      "Iteration: 700\tLog-likelihood: -7.538801896139348\tNum. of topics: 43\n",
      "Iteration: 800\tLog-likelihood: -7.531804622622746\tNum. of topics: 45\n",
      "Iteration: 900\tLog-likelihood: -7.531854914156365\tNum. of topics: 47\n"
     ]
    }
   ],
   "source": [
    "for vec in texts:\n",
    "    hdp.add_doc(vec)\n",
    "hdp.burn_in = 100\n",
    "hdp.train(0)\n",
    "print('Num docs:', len(hdp.docs), ', Vocab size:', hdp.num_vocabs,\n",
    "      ', Num words:', hdp.num_words)\n",
    "print('Removed top words:', hdp.removed_top_words)\n",
    "for i in range(0, 1000, 100):\n",
    "    hdp.train(100) # 100 iterations at a time\n",
    "    print('Iteration: {}\\tLog-likelihood: {}\\tNum. of topics: {}'.format(i, hdp.ll_per_word, hdp.live_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "convertible-expense",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "topics = get_hdp_topics(hdp, top_n=30) # changing top_n changes no. of words displayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-spring",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fatty-milton",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "res = {}\n",
    "id = 0\n",
    "for i in topics.values():\n",
    "    res[id] = []\n",
    "    for j in i:\n",
    "        res[id].append(j[0])\n",
    "    id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-meeting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "neural-bones",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(res).to_csv(\"post-2015.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-elements",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
